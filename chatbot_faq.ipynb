{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51fa6bd-6a36-469f-adbc-d5eff26c5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Vector DB\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Chat\n",
    "import torch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# UI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253a9ddf-1eca-483a-8b19-9bda8a9b0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ABSOLUTE_PATH = os.path.abspath(os.getcwd())\n",
    "VDB_PATH = os.path.join(ABSOLUTE_PATH, \"vector_dbs\")\n",
    "\n",
    "def set_path(db_name: str, vdb_path: str=VDB_PATH) -> str:\n",
    "    return os.path.join(vdb_path, db_name)\n",
    "\n",
    "# FAISS vector store paths\n",
    "FAISSDB_OPENAI_PATH = set_path(\"openai\")\n",
    "FAISSDB_HF_PATH = set_path(\"hugging_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cae474f-8e57-4c27-a702-38856790ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load OpenAI Embeddings\n",
    "load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"type-your-api-key-here\")\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load Hugging Face Embeddings\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"type-your-token-here\")\n",
    "login(HF_TOKEN)\n",
    "hf_embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "if openai_embeddings and hf_embeddings:\n",
    "    print(\"Embeddings are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf6066e-f878-4d1d-beee-eee5b7e47233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:\\Learn\\LLM\\llm_engineering\\my_projects\\nawatech_test\\case2\\vector_dbs\\openai vector stores ...\n",
      "D:\\Learn\\LLM\\llm_engineering\\my_projects\\nawatech_test\\case2\\vector_dbs\\openai vector stores are loaded.\n",
      "Found 11 documents with 1536 dimensions.\n",
      "\n",
      "Loading D:\\Learn\\LLM\\llm_engineering\\my_projects\\nawatech_test\\case2\\vector_dbs\\hugging_face vector stores ...\n",
      "D:\\Learn\\LLM\\llm_engineering\\my_projects\\nawatech_test\\case2\\vector_dbs\\hugging_face vector stores are loaded.\n",
      "Found 11 documents with 384 dimensions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS vectorstore function\n",
    "def load_faiss_db(db_path: str, embeddings) -> FAISS:\n",
    "    if os.path.exists(db_path):\n",
    "        print(f\"Loading {db_path} vector stores ...\")\n",
    "        vectorstores = FAISS.load_local(db_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "        num_docs = vectorstores.index.ntotal\n",
    "        dim = vectorstores.index.d\n",
    "        print(f\"{db_path} vector stores are loaded.\")\n",
    "        print(f\"Found {num_docs} documents with {dim} dimensions.\\n\")\n",
    "        return vectorstores\n",
    "    else:\n",
    "        print(f\"{db_path} not found in directory.\\n\")\n",
    "\n",
    "faiss_openai_vectorstores = load_faiss_db(FAISSDB_OPENAI_PATH, openai_embeddings)\n",
    "faiss_hf_vectorstores = load_faiss_db(FAISSDB_HF_PATH, hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91494d9-b73d-4fae-9a46-eae06ba50b6e",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de56246-cb6a-41cb-87f7-bba0d0791388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama model\n",
    "OLLAMA_API_KEY = \"ollama\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "llama_model = \"llama3.2:latest\"\n",
    "\n",
    "# GPT model\n",
    "gpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559749a3-d555-4974-a005-9f37603e6aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model and its tokenizer are loaded.\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Model\n",
    "hf_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# 1. Load model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "llama_hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    hf_model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# 2. Load model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if llama_hf_model and tokenizer:\n",
    "    print(f\"Hugging Face model and its tokenizer are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8b39a3-23a7-4dde-99ca-72ea16fbbe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini model loaded.\n",
      "<langchain_community.vectorstores.faiss.FAISS object at 0x0000027466A36D10> has been set-up as retriever.\n",
      "chat_history memroy has been set-up.\n",
      "\n",
      "Conversation chain is ready to be tested and used.\n"
     ]
    }
   ],
   "source": [
    "def setup_conversation_chain(model: str, vectorstores, temperature: float):\n",
    "    # 1. Set-up the model\n",
    "    if model == gpt_model:\n",
    "        llm = ChatOpenAI(temperature=temperature, model=gpt_model)\n",
    "        print(f\"{gpt_model} model loaded.\")\n",
    "        \n",
    "    elif model == llama_model:\n",
    "        llm = ChatOpenAI(temperature=temperature, model=llama_model, api_key=OLLAMA_API_KEY, base_url=OLLAMA_BASE_URL)\n",
    "        print(f\"{llama_model} model loaded.\")\n",
    "    \n",
    "    elif model == hf_model or (llama_hf_model is not None and tokenizer is not None):\n",
    "        print(f\"Loading {hf_model} model ...\")\n",
    "        \n",
    "        text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=llama_hf_model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        hf_llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "        print(\"Hugging face pipeline created.\")\n",
    "        \n",
    "        llm = ChatHuggingFace(llm=hf_llm, model_id=hf_model)\n",
    "        print(f\"{hf_model} loaded.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown or invalid model: {model}\")\n",
    "    \n",
    "    # 2. Set-up the retriever: the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "    retriever = vectorstores.as_retriever()\n",
    "    print(f\"{vectorstores} has been set-up as retriever.\")\n",
    "    \n",
    "    # 3. Set-up the conversation memory for the chat\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    print(f\"chat_history memroy has been set-up.\")\n",
    "\n",
    "    # Putting it together: set-up the conversation chain with the GPT 4o-mini or Llama3.2, the vector store and memory\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "    \n",
    "    return conversation_chain\n",
    "\n",
    "# Building custom conversation chain\n",
    "conversation_chain = setup_conversation_chain(\n",
    "    model=gpt_model,\n",
    "    vectorstores=faiss_openai_vectorstores,\n",
    "    temperature=0.7\n",
    ")\n",
    "if conversation_chain:\n",
    "    print(\"\\nConversation chain is ready to be tested and used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc75b96-e3b1-40d1-a00e-1c9f042c6743",
   "metadata": {},
   "source": [
    "# Testing the Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6024a2a-ccf1-4955-9d85-ee903cbc9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini model loaded.\n",
      "<langchain_community.vectorstores.faiss.FAISS object at 0x0000027466A36D10> has been set-up as retriever.\n",
      "chat_history memroy has been set-up.\n"
     ]
    }
   ],
   "source": [
    "# Using specific model and retriever\n",
    "conversation_chain = setup_conversation_chain(\n",
    "    model=gpt_model,\n",
    "    vectorstores=faiss_openai_vectorstores,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "def chat(message, history):\n",
    "    response = conversation_chain.invoke({\"question\": message})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c0a67a-9102-4081-a004-75eaa7ab6925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866db43-eb1f-48b4-8e5a-5df934521bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugface",
   "language": "python",
   "name": "hugface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
